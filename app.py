import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
import tempfile

PROVIDERS = {
    "Groq (ë¬´ë£Œ)": {
        "models": ["llama-3.3-70b-versatile", "llama-3.1-8b-instant", "mixtral-8x7b-32768"],
        "placeholder": "gsk_...",
        "url": "https://console.groq.com",
    },
    "OpenAI": {
        "models": ["gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo"],
        "placeholder": "sk-...",
        "url": "https://platform.openai.com/api-keys",
    },
    "Anthropic (Claude)": {
        "models": ["claude-3-5-sonnet-20241022", "claude-3-5-haiku-20241022", "claude-3-opus-20240229"],
        "placeholder": "sk-ant-...",
        "url": "https://console.anthropic.com",
    },
    "Google Gemini (ë¬´ë£Œ)": {
        "models": ["gemini-2.0-flash", "gemini-1.5-pro", "gemini-1.5-flash"],
        "placeholder": "AIza...",
        "url": "https://aistudio.google.com/apikey",
    },
}

load_dotenv()

st.set_page_config(
    page_title="PDF ì±—ë´‡",
    page_icon="ğŸ“„",
    layout="wide"
)


# â”€â”€ ë¡œê·¸ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def check_login(username, password):
    correct_user = st.secrets.get("LOGIN_USERNAME", "admin")
    correct_pass = st.secrets.get("LOGIN_PASSWORD", "1234")
    return username == correct_user and password == correct_pass


def login_page():
    st.title("ğŸ” PDF ì±—ë´‡")
    st.caption("ë¡œê·¸ì¸ í›„ ì´ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    col1, col2, col3 = st.columns([1, 1.2, 1])
    with col2:
        with st.form("login_form"):
            username = st.text_input("ì•„ì´ë””")
            password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
            submitted = st.form_submit_button("ë¡œê·¸ì¸", use_container_width=True)
            if submitted:
                if check_login(username, password):
                    st.session_state.logged_in = True
                    st.rerun()
                else:
                    st.error("ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")


if not st.session_state.get("logged_in"):
    login_page()
    st.stop()


# â”€â”€ ë©”ì¸ ì•± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.title("ğŸ“„ PDF ë¬¸ì„œ ì±—ë´‡")
st.caption("PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ ìˆì–´ìš”!")


@st.cache_resource(show_spinner="ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
def load_embeddings():
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )


def process_pdfs(uploaded_files):
    all_chunks = []
    total_pages = 0
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

    for uploaded_file in uploaded_files:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.read())
            tmp_path = tmp.name

        loader = PyPDFLoader(tmp_path)
        documents = loader.load()
        total_pages += len(documents)
        chunks = splitter.split_documents(documents)
        all_chunks.extend(chunks)
        os.unlink(tmp_path)

    embeddings = load_embeddings()
    vectorstore = FAISS.from_documents(all_chunks, embeddings)
    return vectorstore, total_pages


def get_api_key():
    user_key = st.session_state.get("user_api_key", "").strip()
    server_key = (os.getenv("GROQ_API_KEY") or st.secrets.get("GROQ_API_KEY", "")).strip()
    return user_key if user_key else server_key


def build_llm(provider, model, api_key):
    if provider == "Groq (ë¬´ë£Œ)":
        return ChatGroq(model=model, temperature=0, api_key=api_key, streaming=True)
    elif provider == "OpenAI":
        return ChatOpenAI(model=model, temperature=0, api_key=api_key, streaming=True)
    elif provider == "Anthropic (Claude)":
        return ChatAnthropic(model=model, temperature=0, api_key=api_key, streaming=True)
    elif provider == "Google Gemini (ë¬´ë£Œ)":
        return ChatGoogleGenerativeAI(model=model, temperature=0, google_api_key=api_key, streaming=True)


def build_chain():
    api_key = get_api_key()
    provider = st.session_state.get("selected_provider", "Groq (ë¬´ë£Œ)")
    model = st.session_state.get("selected_model", "llama-3.3-70b-versatile")
    llm = build_llm(provider, model, api_key)
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ 'ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë§í•´ì£¼ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]
{context}"""),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}")
    ])
    return prompt | llm


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


def format_history(messages):
    history = []
    for msg in messages:
        if msg["role"] == "user":
            history.append(HumanMessage(content=msg["content"]))
        else:
            history.append(AIMessage(content=msg["content"]))
    return history


def stream_response(chain, retriever, question, messages):
    context_docs = retriever.invoke(question)
    context = format_docs(context_docs)
    history = format_history(messages)

    for chunk in chain.stream({
        "context": context,
        "question": question,
        "chat_history": history
    }):
        yield chunk.content

    return context_docs


# â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    # AI í”„ë¡œë°”ì´ë” ì„¤ì •
    with st.expander("ğŸ¤– AI ì„¤ì •", expanded=not bool(get_api_key())):
        provider = st.selectbox(
            "AI ì œê³µì‚¬ ì„ íƒ",
            options=list(PROVIDERS.keys()),
            index=list(PROVIDERS.keys()).index(st.session_state.get("selected_provider", "Groq (ë¬´ë£Œ)"))
        )
        st.session_state.selected_provider = provider

        model = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            options=PROVIDERS[provider]["models"],
            index=0
        )
        st.session_state.selected_model = model

        api_key_input = st.text_input(
            "API í‚¤",
            type="password",
            placeholder=PROVIDERS[provider]["placeholder"],
            value=st.session_state.get("user_api_key", ""),
        )
        st.caption(f"í‚¤ ë°œê¸‰ â†’ [{provider}]({PROVIDERS[provider]['url']})")

        if api_key_input:
            st.session_state.user_api_key = api_key_input.strip()
            st.success("âœ… í‚¤ ì €ì¥ë¨")

    st.divider()
    st.header("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")

    uploaded_files = st.file_uploader(
        "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
        type="pdf",
        accept_multiple_files=True
    )

    if uploaded_files:
        if st.button("ğŸ“¥ ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°", use_container_width=True):
            if not get_api_key():
                st.error("API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”!")
            else:
                with st.spinner(f"{len(uploaded_files)}ê°œ ë¬¸ì„œ ë¶„ì„ ì¤‘..."):
                    vectorstore, total_pages = process_pdfs(uploaded_files)
                    st.session_state.vectorstore = vectorstore
                    st.session_state.retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
                    st.session_state.chain = build_chain()
                    st.session_state.messages = []
                    names = ", ".join(f.name for f in uploaded_files)
                    st.success(f"ì™„ë£Œ! {len(uploaded_files)}ê°œ íŒŒì¼ / ì´ {total_pages}í˜ì´ì§€\n\nğŸ“„ {names}")

    if "vectorstore" in st.session_state:
        st.divider()
        st.success("âœ… ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ")
        if st.button("ğŸ—‘ï¸ ì´ˆê¸°í™”", use_container_width=True):
            for key in ["vectorstore", "retriever", "chain", "messages"]:
                st.session_state.pop(key, None)
            st.rerun()

    st.divider()
    if st.button("ğŸšª ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.rerun()


# â”€â”€ ì±„íŒ… ì˜ì—­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if "messages" not in st.session_state:
    st.session_state.messages = []

if "chain" not in st.session_state:
    if not get_api_key():
        st.warning("ğŸ‘ˆ ì™¼ìª½ì—ì„œ **Groq API í‚¤**ë¥¼ ì…ë ¥í•˜ê³  PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.info("Groq API í‚¤ê°€ ì—†ë‹¤ë©´ â†’ https://console.groq.com ì—ì„œ ë¬´ë£Œë¡œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”!")
    else:
        st.info("ğŸ‘ˆ ì™¼ìª½ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
else:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        with st.chat_message("assistant"):
            response_chunks = []

            def collect_and_stream():
                for chunk in stream_response(
                    st.session_state.chain,
                    st.session_state.retriever,
                    prompt,
                    st.session_state.messages[:-1]
                ):
                    response_chunks.append(chunk)
                    yield chunk

            try:
                st.write_stream(collect_and_stream())
                full_response = "".join(response_chunks)
            except Exception as e:
                err = str(e)
                if "429" in err or "RESOURCE_EXHAUSTED" in err or "quota" in err.lower():
                    full_response = "âš ï¸ API ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ AI ì œê³µì‚¬ë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”."
                elif "401" in err or "invalid" in err.lower() or "authentication" in err.lower():
                    full_response = "âš ï¸ API í‚¤ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                elif "404" in err or "model" in err.lower():
                    full_response = "âš ï¸ ì„ íƒí•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
                else:
                    full_response = f"âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {err[:200]}"
                st.warning(full_response)

        st.session_state.messages.append({"role": "assistant", "content": full_response})
