import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import tempfile

load_dotenv()

# í‚¤ì— ì„ì¸ ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°
if os.getenv("GROQ_API_KEY"):
    os.environ["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY").strip()

st.set_page_config(
    page_title="PDF ì±—ë´‡",
    page_icon="ğŸ“„",
    layout="wide"
)

st.title("ğŸ“„ PDF ë¬¸ì„œ ì±—ë´‡")
st.caption("PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ ìˆì–´ìš”!")


@st.cache_resource(show_spinner="ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
def load_embeddings():
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )


def process_pdf(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    loader = PyPDFLoader(tmp_path)
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = splitter.split_documents(documents)

    embeddings = load_embeddings()
    vectorstore = FAISS.from_documents(chunks, embeddings)

    os.unlink(tmp_path)
    return vectorstore, len(documents)


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


def format_history(chat_history):
    messages = []
    for msg in chat_history:
        if msg["role"] == "user":
            messages.append(HumanMessage(content=msg["content"]))
        else:
            messages.append(AIMessage(content=msg["content"]))
    return messages


def get_answer(chain, retriever, question, chat_history):
    context_docs = retriever.invoke(question)
    context = format_docs(context_docs)
    history = format_history(chat_history)

    answer = chain.invoke({
        "context": context,
        "question": question,
        "chat_history": history
    })
    return answer, context_docs


def build_chain():
    llm = ChatGroq(
        model="llama-3.3-70b-versatile",
        temperature=0,
        api_key=os.getenv("GROQ_API_KEY")
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ 'ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë§í•´ì£¼ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]
{context}"""),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}")
    ])

    chain = prompt | llm | StrOutputParser()
    return chain


# ì‚¬ì´ë“œë°” - PDF ì—…ë¡œë“œ
with st.sidebar:
    st.header("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type="pdf")

    if uploaded_file:
        if st.button("ğŸ“¥ ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°", use_container_width=True):
            with st.spinner("ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘..."):
                vectorstore, page_count = process_pdf(uploaded_file)
                st.session_state.vectorstore = vectorstore
                st.session_state.retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
                st.session_state.chain = build_chain()
                st.session_state.messages = []
                st.success(f"ì™„ë£Œ! ì´ {page_count}í˜ì´ì§€ ì²˜ë¦¬ë¨")

    if "vectorstore" in st.session_state:
        st.divider()
        st.success("âœ… ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ")
        if st.button("ğŸ—‘ï¸ ì´ˆê¸°í™”", use_container_width=True):
            for key in ["vectorstore", "retriever", "chain", "messages"]:
                st.session_state.pop(key, None)
            st.rerun()


# ë©”ì¸ ì±„íŒ… ì˜ì—­
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chain" not in st.session_state:
    st.info("ğŸ‘ˆ ì™¼ìª½ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
else:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                answer, source_docs = get_answer(
                    st.session_state.chain,
                    st.session_state.retriever,
                    prompt,
                    st.session_state.messages[:-1]
                )
                st.write(answer)

                with st.expander("ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ ë‚´ìš©"):
                    for i, doc in enumerate(source_docs, 1):
                        st.caption(f"[{i}] í˜ì´ì§€ {doc.metadata.get('page', '?') + 1}")
                        st.text(doc.page_content[:300] + "...")

        st.session_state.messages.append({"role": "assistant", "content": answer})
